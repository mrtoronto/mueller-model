{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Loading-and-Cleaning-Data\" data-toc-modified-id=\"Loading-and-Cleaning-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading and Cleaning Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Clean-Raw-Text\" data-toc-modified-id=\"Clean-Raw-Text-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clean Raw Text</a></span></li><li><span><a href=\"#Character-Mappings\" data-toc-modified-id=\"Character-Mappings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Character Mappings</a></span></li></ul></li><li><span><a href=\"#Creating-train-and-test-data-objects\" data-toc-modified-id=\"Creating-train-and-test-data-objects-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating train and test data objects</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-Generation-Functions\" data-toc-modified-id=\"Dataset-Generation-Functions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dataset Generation Functions</a></span></li><li><span><a href=\"#Creating-Datasets\" data-toc-modified-id=\"Creating-Datasets-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating Datasets</a></span></li></ul></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#750-Sequence-Length-Model\" data-toc-modified-id=\"750-Sequence-Length-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>750 Sequence Length Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Set-up-Checkpoint-Path\" data-toc-modified-id=\"Set-up-Checkpoint-Path-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Set up Checkpoint Path</a></span></li><li><span><a href=\"#Build-model\" data-toc-modified-id=\"Build-model-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Build model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#1-Batch-Model-and-Imputing-Redactions\" data-toc-modified-id=\"1-Batch-Model-and-Imputing-Redactions-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>1 Batch Model and Imputing Redactions</a></span></li></ul></li><li><span><a href=\"#400-Sequence-Length-Model\" data-toc-modified-id=\"400-Sequence-Length-Model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>400 Sequence Length Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Set-up-Checkpoint-Path\" data-toc-modified-id=\"Set-up-Checkpoint-Path-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Set up Checkpoint Path</a></span></li><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Build Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#1-Batch-and-Redactions\" data-toc-modified-id=\"1-Batch-and-Redactions-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>1 Batch and Redactions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "The model will be made in TensorFlow using [Eager Execution mode](https://www.tensorflow.org/guide/eager). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T16:52:15.257934Z",
     "start_time": "2019-08-13T16:52:15.253901Z"
    }
   },
   "outputs": [],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning Data\n",
    "\n",
    "\n",
    "### Load Data\n",
    "Load the raw_text. I frequently trained models using google colab so I have an option to mount and load a google drive folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T14:11:01.617011Z",
     "start_time": "2019-08-13T14:11:01.613773Z"
    }
   },
   "outputs": [],
   "source": [
    "### Option to run notebook using google colab as data storage\n",
    "drive_load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T12:56:55.235923Z",
     "start_time": "2019-08-13T12:56:55.185891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '§', '¶', 'é', 'ü', 'ſ', 'а', 'в', 'д', 'е', 'и', 'к', 'м', 'н', 'о', 'п', 'р', 'т', 'ч', 'ы', 'я', '–', '—', '‘', '’', '“', '”', '…', '■']\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    path_to_file = \"/content/gdrive/My Drive/code/redaction model/strippedtext.txt\"\n",
    "else:\n",
    "    path_to_file = \"strippedtext.txt\"\n",
    "\n",
    "\n",
    "raw_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "unfiltered_vocab = sorted(set(raw_text))\n",
    "print(unfiltered_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Raw Text\n",
    "\n",
    "Remove a bunch of miscellaneous characters from the raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T13:01:47.004935Z",
     "start_time": "2019-08-13T13:01:46.936381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text pre-substituion : 1320770 characters.\n",
      "120 unique characters after substitution\n",
      "\n",
      "Length of text post-substituion : 1304546 characters. \n",
      "76 unique characters after substitution. \n",
      "\n",
      "Pre-to-Post difference : 16224 characters.\n"
     ]
    }
   ],
   "source": [
    "print('Length of text pre-substituion : {} characters.'.format(len(raw_text)))\n",
    "print('{} unique characters after substitution'.format(len(unfiltered_vocab)))\n",
    "\n",
    "sub_text = re.sub('[!@#$§\\[\\]%&+*\\(\\);<>=|¶éüſавдеикмнопртчыя–—‘’“”]', '', raw_text)\n",
    "sub_text = re.sub('…', '...', sub_text)\n",
    "\n",
    "vocab = sorted(set(sub_text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('\\nLength of text post-substituion : {} characters. '.format(len(sub_text)))\n",
    "print('{} unique characters after substitution. \\n\\nPre-to-Post difference : {} characters.'.format(len(vocab), len(raw_text) - len(sub_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Mappings\n",
    "\n",
    "Create mappings from characters to indices and from indices to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T13:05:05.744423Z",
     "start_time": "2019-08-13T13:05:05.738432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '\"' :   3,\n",
      "  \"'\" :   4,\n"
     ]
    }
   ],
   "source": [
    "### Create a mapping from unique characters to indices and vice versa\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "for char,_ in zip(char2idx, range(5)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T13:05:43.879560Z",
     "start_time": "2019-08-13T13:05:43.873333Z"
    }
   },
   "source": [
    "## Creating train and test data objects\n",
    "\n",
    "Create the training and test text sets.\n",
    "- The training set will be all the text from the original report with any redactions sections removed and replaced with three periods (...).\n",
    "- The test set will be made of all the redactions in the report and some number of preceding characters to prompt the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:05:20.646847Z",
     "start_time": "2019-08-13T18:05:20.565824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\\r\\nEXECUTIVE SUMMARY TO VOLUME I\\r\\nRUSSIAN SOCIAL MEDIA CAMPAIGN\\r\\nThe Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■', 'TIVE SUMMARY TO VOLUME I\\r\\nRUSSIAN SOCIAL MEDIA CAMPAIGN\\r\\nThe Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\\r\\nIn mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■']\n"
     ]
    }
   ],
   "source": [
    "train_text = re.sub('■+', '...',sub_text)\n",
    "\n",
    "### Each redaction has the first and last index values recorded along with the \n",
    "### length of the redactions\n",
    "### Length of redactions isn't actually used anywhere\n",
    "redactions = [[m.start(), m.end(), m.end() - m.start()] for m in re.finditer('■+', sub_text)]\n",
    "\n",
    "prec_char_750 = 750\n",
    "prec_char_400 = 400\n",
    "\n",
    "test_text_750 = [sub_text[i[0] - preceding_characters : i[1]] for i in redactions]\n",
    "test_text_400 = [sub_text[i[0] - preceding_characters : i[1]] for i in redactions]\n",
    "\n",
    "print(test_text_400[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text into the numeric array form using the character to index mappings made earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:06:01.524095Z",
     "start_time": "2019-08-13T18:06:00.790090Z"
    }
   },
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in sub_text])\n",
    "train_text_as_int = np.array([char2idx[c] for c in train_text])\n",
    "test_text_750_as_int = [np.array([char2idx[c] for c in i]) for i in test_text_750]\n",
    "test_text_400_as_int = [np.array([char2idx[c] for c in i]) for i in test_text_400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation Functions\n",
    "\n",
    "Create a function to chunk up an input string, splitting it into predictor text and a target character. \n",
    "\n",
    "Batch the full text into seq_length + 1 length sequences then `map` that new function onto the sequences to produce predictors and target chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:21:35.387777Z",
     "start_time": "2019-08-13T17:21:35.382750Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "def dataset_generator(seq_length, text):\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(train_text_as_int)\n",
    "    \n",
    "    ### Increasing to 100 prevented model from \"running out of data\". \n",
    "    ### Probably part of ResourceExhausted Error but running out of data was worse.\n",
    "    repeat_n = 100\n",
    "    sequences = char_dataset.repeat(repeat_n).batch(seq_length+1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with sequence lengths 750 and 400 to test out two different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:07:41.429126Z",
     "start_time": "2019-08-13T18:07:41.401369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((400,), (400,)), types: (tf.int64, tf.int64)> \n",
      " <DatasetV1Adapter shapes: ((750,), (750,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "seq_length_400 = 400\n",
    "dataset_400 = dataset_generator(seq_length_400, sub_text)\n",
    "\n",
    "seq_length_750 = 750\n",
    "dataset_750 = dataset_generator(seq_length_750, sub_text)\n",
    "print(dataset_400, '\\n', dataset_750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "First I'll define some variables and functions that'll be used in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:16:52.235718Z",
     "start_time": "2019-08-13T18:16:52.233248Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function for the models. We'll use categorical crossentropy as that's the simplest loss function for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:17:52.092370Z",
     "start_time": "2019-08-13T18:17:52.088112Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text given a starting string using a model. Code originally taken from [this](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb) TensorFlow tutorial but adapted to this use case.\n",
    "\n",
    "- Changes include:\n",
    "    - Making `temperature` a parameter\n",
    "    - Changing `num_generate` to dynamically set to the length of redacted piece of `start_string`\n",
    "    - Add a separator in the text generated to show where redaction/prediction starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:49:30.349156Z",
     "start_time": "2019-08-13T17:49:30.340632Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature=1):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = len(start_string) - len(re.sub('■+', '', start_string))\n",
    "    start_string = re.sub('■+', '', start_string)\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    #temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ' || ' + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 750 Sequence Length Model\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "Set parameters for 750 sequence length model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:08:08.065865Z",
     "start_time": "2019-08-13T18:08:08.060926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((64, 750), (64, 750)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "learning_rate_750 = 5e-4\n",
    "embedding_dim_750 = 256\n",
    "nb_epoches_750 = 1\n",
    "rnn_units_750 = 600\n",
    "load_weights_750 = True\n",
    "keep_training_750 = False\n",
    "\n",
    "### Not sure if is doing what I think but it was used in a tutorial I followed so I'm using it\n",
    "### Essentially, I want each epoch to read the document once\n",
    "examples_per_epoch_750 = len(sub_text)//seq_length_750\n",
    "steps_per_epoch_750 = int(examples_per_epoch / BATCH_SIZE)\n",
    "\n",
    "### Set up dataset that's been shuffled and batched\n",
    "dataset_sb_750 = dataset_750.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_sb_750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the user is loading from drive or now, pick the folder that'll have the checkpoints in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:25:55.946511Z",
     "start_time": "2019-08-13T17:25:55.942022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_750_model\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    checkpoint_750_path = \"/content/gdrive/My Drive/code/redaction model/checkpoints_750_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "else:\n",
    "    checkpoint_750_path = \"checkpoints_750_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "    \n",
    "checkpoint_750_dir = os.path.dirname(checkpoint_750_path)\n",
    "\n",
    "### Make the directory if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_750_dir):\n",
    "    os.mkdir(checkpoint_750_dir)\n",
    "    \n",
    "print(checkpoint_750_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "Build the model for the 750 sequence length model. \n",
    "\n",
    "Use only 1 GRU because that's as big as Google Colab will let it get while in Eager Execution mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:03.244103Z",
     "start_time": "2019-08-13T17:26:03.238343Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_750(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5), \n",
    "        tf.keras.layers.Dense(2*vocab_size),\n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model object, compile it and potentially load weights to it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:10.895034Z",
     "start_time": "2019-08-13T17:26:10.497941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from :  checkpoints_750_model/epochs:001-loss:1.176.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (64, None, 256)           19456     \n",
      "_________________________________________________________________\n",
      "gru_31 (GRU)                 (64, None, 600)           1542600   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (64, None, 152)           91352     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (64, None, 76)            11628     \n",
      "=================================================================\n",
      "Total params: 1,665,036\n",
      "Trainable params: 1,665,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_750 = build_model_750(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim_750,\n",
    "    rnn_units=rnn_units_750,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "adam_opt = Adam(lr=learning_rate_750)\n",
    "\n",
    "model_750.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "most_recent_750_checkpoint = max([checkpoint_750_dir + '/' + i for i in os.listdir(checkpoint_750_dir)], key = os.path.getctime)\n",
    "\n",
    "if load_weights_750 == True:\n",
    "    model_750.load_weights(most_recent_750_checkpoint)\n",
    "    print(\"Weights loaded from : \", most_recent_750_checkpoint)\n",
    "\n",
    "model_750.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model\n",
    "\n",
    "Add callbacks then fit the model.\n",
    "- If `load_weights` == True then the user has weights they would like to load prior to training.\n",
    "- If `keep_training` == True then the user wants the model to train further after starting from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:45.985864Z",
     "start_time": "2019-08-13T17:26:18.260628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 27s 27s/step - loss: 1.3378\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_750_path, period=1,\n",
    "                                                         monitor='loss', save_best_only=True, \n",
    "                                                         mode='min')\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint_callback, earlystop_callback]\n",
    "\n",
    "if load_weights_750 == True:\n",
    "    if keep_training_750 == True:\n",
    "        history = model_750.fit(\n",
    "                            dataset_750, \n",
    "                            epochs=nb_epoches_750, \n",
    "                            steps_per_epoch=steps_per_epoch_750, \n",
    "                            callbacks=callbacks_list)\n",
    "    elif keep_training_750 == False:\n",
    "        history = model_750.fit(\n",
    "                            dataset_750, \n",
    "                            epochs=1, \n",
    "                            steps_per_epoch=1, \n",
    "                            callbacks=callbacks_list)\n",
    "else:\n",
    "    history = model_750.fit(\n",
    "                        dataset_750, \n",
    "                        epochs=nb_epoches_750, \n",
    "                        steps_per_epoch=steps_per_epoch_750, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Batch Model and Imputing Redactions\n",
    "\n",
    "Build a model with 1 batch size and load weights from above cell's checkpoints. \n",
    "\n",
    "Using this 1batch model, generate predictions for the first 5 redactions of the Mueller report.\n",
    "- Generate three predictions with varying temperatures per redaction.\n",
    "    - Lower temperature gives less predictable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T19:15:00.891266Z",
     "start_time": "2019-08-13T19:14:58.240178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_750_model/epochs:001-loss:1.176.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (1, None, 256)            19456     \n",
      "_________________________________________________________________\n",
      "gru_41 (GRU)                 (1, None, 600)            1542600   \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (1, None, 152)            91352     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (1, None, 76)             11628     \n",
      "=================================================================\n",
      "Total params: 1,665,036\n",
      "Trainable params: 1,665,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "750 Sequence Length predictors\n",
      "\n",
      "################################################\n",
      "\n",
      "0\n",
      "###\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-80caf1209521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_750_1batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_750_1batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-c6213178168d>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, temperature)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# remove the batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     outputs, _ = self._call_and_compute_mask(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    813\u001b[0m     outputs, _ = self._run_internal_graph(inputs,\n\u001b[1;32m    814\u001b[0m                                           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m                                           mask=masks)\n\u001b[0m\u001b[1;32m    816\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    998\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                   \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                   \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recurrent_dropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m     return super(GRU, self).call(\n\u001b[0;32m-> 1797\u001b[0;31m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         zero_output_for_mask=self.zero_output_for_mask)\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   3569\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3571\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   3572\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3531\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3532\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3523\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3524\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   3554\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m         output, new_states = step_function(current_input,\n\u001b[0;32m-> 3556\u001b[0;31m                                            tuple(states) + tuple(constants))\n\u001b[0m\u001b[1;32m   3557\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_rnn_cell\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m           \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0mh_tm1_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_tm1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m       \u001b[0mrecurrent_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m       recurrent_r = K.dot(h_tm1_r,\n\u001b[1;32m   1564\u001b[0m                           self.recurrent_kernel[:, self.units:self.units * 2])\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_SliceHelperVar\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m    891\u001b[0m   \"\"\"\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_slice_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   9316\u001b[0m         \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9317\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shrink_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9318\u001b[0;31m         shrink_axis_mask)\n\u001b[0m\u001b[1;32m   9319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9320\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(most_recent_750_checkpoint)\n",
    "\n",
    "model_750_1batch = build_model_750(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model_750_1batch.load_weights(most_recent_750_checkpoint)\n",
    "model_750_1batch.build(tf.TensorShape([1, None]))\n",
    "model_750_1batch.summary()\n",
    "\n",
    "print(\"750 Sequence Length predictors\")\n",
    "for index, redaction in enumerate(test_text[:5]):\n",
    "    print(\"\\n################################################\\n\")\n",
    "    print(index)\n",
    "    print(\"###\")\n",
    "    print(redaction)\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=1))\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=.3))\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 Sequence Length Model\n",
    "\n",
    "\n",
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:08:44.359316Z",
     "start_time": "2019-08-13T18:08:44.354041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((64, 400), (64, 400)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "learning_rate_400 = 5e-4\n",
    "embedding_dim_400 = 256\n",
    "nb_epoches_400 = 1\n",
    "rnn_units_400 = 600\n",
    "load_weights_400 = True\n",
    "keep_training_400 = False\n",
    "examples_per_epoch_400 = len(sub_text)//seq_length_400\n",
    "steps_per_epoch_400 = int(examples_per_epoch / BATCH_SIZE)\n",
    "\n",
    "\n",
    "dataset_sb_400 = dataset_400.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_sb_400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:08:51.682903Z",
     "start_time": "2019-08-13T18:08:51.676871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_400_model\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    checkpoint_400_path = \"/content/gdrive/My Drive/code/redaction model/checkpoints_400_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "else:\n",
    "    checkpoint_400_path = \"checkpoints_400_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "    \n",
    "checkpoint_400_dir = os.path.dirname(checkpoint_400_path)\n",
    "\n",
    "if not os.path.isdir(checkpoint_400_dir):\n",
    "    os.mkdir(checkpoint_400_dir)\n",
    "    \n",
    "print(checkpoint_400_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:08:58.889156Z",
     "start_time": "2019-08-13T18:08:58.882816Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_400(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5),         \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5), \n",
    "        tf.keras.layers.Dense(2*vocab_size),\n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:51:12.343196Z",
     "start_time": "2019-08-13T17:51:11.755216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from :  checkpoints_400_model/epochs_015-loss_1.294.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (64, None, 256)           19456     \n",
      "_________________________________________________________________\n",
      "gru_36 (GRU)                 (64, None, 600)           1542600   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "gru_37 (GRU)                 (64, None, 600)           2161800   \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (64, None, 152)           91352     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (64, None, 76)            11628     \n",
      "=================================================================\n",
      "Total params: 3,826,836\n",
      "Trainable params: 3,826,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_400 = build_model_400(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim_400,\n",
    "    rnn_units=rnn_units_400,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "adam_opt = Adam(lr=learning_rate_400)\n",
    "\n",
    "model_400.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "most_recent_400_checkpoint = max([checkpoint_400_dir + '/' + i for i in os.listdir(checkpoint_400_dir)], key = os.path.getctime)\n",
    "\n",
    "if load_weights_400 == True:\n",
    "    model_400.load_weights(most_recent_400_checkpoint)\n",
    "    print(\"Weights loaded from : \", most_recent_400_checkpoint)\n",
    "\n",
    "model_400.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:51:50.671873Z",
     "start_time": "2019-08-13T17:51:19.502125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 30s 30s/step - loss: 1.3041\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_400_path, period=1,\n",
    "                                                         monitor='loss', save_best_only=True, \n",
    "                                                         mode='min')\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint_callback, earlystop_callback]\n",
    "\n",
    "if load_weights_400 == True:\n",
    "    if keep_training_400 == True:\n",
    "        history = model_400.fit(\n",
    "                            dataset_400, \n",
    "                            epochs=nb_epoches_400, \n",
    "                            steps_per_epoch=steps_per_epoch_400, \n",
    "                            callbacks=callbacks_list)\n",
    "    elif keep_training_400 == False:\n",
    "        history = model_400.fit(\n",
    "                            dataset_400, \n",
    "                            epochs=1, \n",
    "                            steps_per_epoch=1, \n",
    "                            callbacks=callbacks_list)\n",
    "else:\n",
    "    history = model_400.fit(\n",
    "                        dataset_400, \n",
    "                        epochs=nb_epoches_400, \n",
    "                        steps_per_epoch=steps_per_epoch_400, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Batch and Redactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:55:36.616075Z",
     "start_time": "2019-08-13T17:52:32.217397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_400_model/epochs_015-loss_1.294.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (1, None, 256)            19456     \n",
      "_________________________________________________________________\n",
      "gru_39 (GRU)                 (1, None, 600)            1542600   \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "gru_40 (GRU)                 (1, None, 600)            2161800   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (1, None, 152)            91352     \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (1, None, 76)             11628     \n",
      "=================================================================\n",
      "Total params: 3,826,836\n",
      "Trainable params: 3,826,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "400 Sequence Length predictors\n",
      "\n",
      "################################################\n",
      "\n",
      "0\n",
      "###\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || not curtain in Sessions work not tixe Russian Ereacy, 575 She 6/6/17 302, at 4.\n",
      "444 Based however, Sessions was information with the \n",
      "###\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || the Presidents protection in the 2017 the Rethrets followed, Russia investigation reside contacts first it of the reported that the We\n",
      "###\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || Moed not fund prosecution with by mideto President on No. Compubort, the email ancounted instaa 504 whine it. The President was standa\n",
      "\n",
      "################################################\n",
      "\n",
      "1\n",
      "###\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || statute view to Congress actions of the investigation. 175\n",
      "On the supmoniby in 2:14. a\n",
      "3. a criminal investigation. Trump responded to get the Presidents interview that Marrial Interference in the campaign furgher the resalt, the Presidents counsel that Office helantorald Gates was a was constitute mefting that the Presidents persons apreading for the laters of a tluigh-pablicly recalled act stolen Priebus, summar in Chint through explaind. 157 Potential briccing additionor, however,\n",
      "###\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || with the Russian ambassadors is wes sated in a cussion had stolen government related to respince that when the attorney as an information be assistent and a hrecgen, on time of as Comey of court, but consalions other countelly information charging prosecutority of these assessipre for office caselowing a mealsign lader in the campaign to vern wlith to resugn anl get with Rissiafk. 991 The Office that McGahn well you toverrews from a Russian only at continque , CNI Mae Corti of Russian \n",
      "###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || has pursued to meet this investigated, whith 630. Wexe was deail meeting at the NII After comnesceed ynot the next statomed arout tto of public inforration and that the Campeog, Gording Comey, and proposemthed by personal that jury include an intend the President alds candidate Washn as SaminJ Volume I, Section II ...\n",
      "A.D. The Special Counsels Office that see Crimyion of 795 McGahn and McGahn attended the Onga investigated by guirty or\n",
      "Cohen s-AMD, Sectors After the Russia investigat\n",
      "\n",
      "################################################\n",
      "\n",
      "2\n",
      "###\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016, ■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || the President also detr\n",
      "###\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || te be subsetted nither \n",
      "###\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || invitation actorion con\n",
      "\n",
      "################################################\n",
      "\n",
      "3\n",
      "###\n",
      "fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016, ■■■■■■■■■■■■■■■■■■■■■■■ forecast to senior Campaign officials that WikiLeaks would release information damaging to candidate Clinton. WikiLeakss first release came in July 2016. Around the same time, candidate Trump announced that he hoped Russia would recover emails described as missing from a private server used by Clinton when she was Secretary of State he later said that he was speaking sarcastically. ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n",
      "fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  forecast to senior Campaign officials that WikiLeaks would release information damaging to candidate Clinton. WikiLeakss first release came in July 2016. Around the same time, candidate Trump announced that he hoped Russia would recover emails described as missing from a private server used by Clinton when she was Secretary of State he later said that he was speaking sarcastically.  || Assected to the Flenc policyaria and Trump and adthorigy record of tite, and ad just corts-provided vater team lawar  on Russia Natzoria, SAs. notice, response to not were appointment fubrociaged in the Russian Interference If Jescomperse informa\n",
      "###\n",
      "fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  forecast to senior Campaign officials that WikiLeaks would release information damaging to candidate Clinton. WikiLeakss first release came in July 2016. Around the same time, candidate Trump announced that he hoped Russia would recover emails described as missing from a private server used by Clinton when she was Secretary of State he later said that he was speaking sarcastically.  || I9 a more including the Russian ilintry and message, Judiciary that hisser contacts with Millian fional Counsel were conlucted want to difly the stole dourt comploase refornal beorgel permoning the Oftly testion.\n",
      "1021 Comey 1/16/18 302, at 7-8. \n",
      "###\n",
      "fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  forecast to senior Campaign officials that WikiLeaks would release information damaging to candidate Clinton. WikiLeakss first release came in July 2016. Around the same time, candidate Trump announced that he hoped Russia would recover emails described as missing from a private server used by Clinton when she was Secretary of State he later said that he was speaking sarcastically.  || And the grand jury purpased out is with the same crose of such act of a crance of the said he was to and and to the effect those court officials, statements thing in and it told ... and that discussing his proof. Actord the President to advestion\n",
      "\n",
      "################################################\n",
      "\n",
      "4\n",
      "###\n",
      "and entities involved in the social media campaign have been charged with participating in a conspiracy to defraud the United States by undermining through deceptive acts the work of federal agencies charged with regulating foreign influence in U.S. elections, as well as related counts of identity theft. See United States v. Internet Research Agency, et al. , No. 18-cr-32 D.D.C.. Separately, Russian intelligence officers who carried out the hacking into Democratic Party computers and the personal email accounts of individuals affiliated with the Clinton Campaign conspired to violate, among other federal laws, the federal computer-intrusion statute, and they have been so charged. See United States v. Netyksho, et al. , No. 18-cr-215 D.D.C.. ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and entities involved in the social media campaign have been charged with participating in a conspiracy to defraud the United States by undermining through deceptive acts the work of federal agencies charged with regulating foreign influence in U.S. elections, as well as related counts of identity theft. See United States v. Internet Research Agency, et al. , No. 18-cr-32 D.D.C.. Separately, Russian intelligence officers who carried out the hacking into Democratic Party computers and the personal email accounts of individuals affiliated with the Clinton Campaign conspired to violate, among other federal laws, the federal computer-intrusion statute, and they have been so charged. See United States v. Netyksho, et al. , No. 18-cr-215 D.D.C..  || Presidential Candidate, July 11, 2017.\n",
      "788 Comey Manafort, he would rightaired theme in tumpore entity perporled, justice. In also AR\n",
      "###\n",
      "and entities involved in the social media campaign have been charged with participating in a conspiracy to defraud the United States by undermining through deceptive acts the work of federal agencies charged with regulating foreign influence in U.S. elections, as well as related counts of identity theft. See United States v. Internet Research Agency, et al. , No. 18-cr-32 D.D.C.. Separately, Russian intelligence officers who carried out the hacking into Democratic Party computers and the personal email accounts of individuals affiliated with the Clinton Campaign conspired to violate, among other federal laws, the federal computer-intrusion statute, and they have been so charged. See United States v. Netyksho, et al. , No. 18-cr-215 D.D.C..  || 875 St. PT Lad Primorzon/liDes of XSat Messoote, McGahn. Strenview Burouly, Surger Special Counsels information was rofe from attropre\n",
      "###\n",
      "and entities involved in the social media campaign have been charged with participating in a conspiracy to defraud the United States by undermining through deceptive acts the work of federal agencies charged with regulating foreign influence in U.S. elections, as well as related counts of identity theft. See United States v. Internet Research Agency, et al. , No. 18-cr-32 D.D.C.. Separately, Russian intelligence officers who carried out the hacking into Democratic Party computers and the personal email accounts of individuals affiliated with the Clinton Campaign conspired to violate, among other federal laws, the federal computer-intrusion statute, and they have been so charged. See United States v. Netyksho, et al. , No. 18-cr-215 D.D.C..  || Net Novs 6/Officeart 8/15/18 Notes 4/29/18 302, at 30. 207 Barrod that to Spearh Sengey Bost that after Office for the cammaigr after \n"
     ]
    }
   ],
   "source": [
    "print(most_recent_400_checkpoint)\n",
    "\n",
    "model_400_1batch = build_model_400(vocab_size, embedding_dim_400, rnn_units_400, batch_size=1)\n",
    "model_400_1batch.load_weights(most_recent_400_checkpoint)\n",
    "model_400_1batch.build(tf.TensorShape([1, None]))\n",
    "model_400_1batch.summary()\n",
    "\n",
    "print(\"400 Sequence Length predictors\")\n",
    "for index, redaction in enumerate(test_text[:5]):\n",
    "    print(\"\\n################################################\\n\")\n",
    "    print(index)\n",
    "    print(\"###\")\n",
    "    print(redaction)\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction))\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction))\n",
    "    print(\"###\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
