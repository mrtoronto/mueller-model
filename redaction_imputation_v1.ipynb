{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Loading-and-Cleaning-Data\" data-toc-modified-id=\"Loading-and-Cleaning-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading and Cleaning Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Clean-Raw-Text\" data-toc-modified-id=\"Clean-Raw-Text-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clean Raw Text</a></span></li><li><span><a href=\"#Character-Mappings\" data-toc-modified-id=\"Character-Mappings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Character Mappings</a></span></li></ul></li><li><span><a href=\"#Creating-train-and-test-data-objects\" data-toc-modified-id=\"Creating-train-and-test-data-objects-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating train and test data objects</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-Generation-Functions\" data-toc-modified-id=\"Dataset-Generation-Functions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dataset Generation Functions</a></span></li><li><span><a href=\"#Creating-Datasets\" data-toc-modified-id=\"Creating-Datasets-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating Datasets</a></span></li></ul></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#750-Sequence-Length-Model\" data-toc-modified-id=\"750-Sequence-Length-Model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>750 Sequence Length Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Set-up-Checkpoint-Path\" data-toc-modified-id=\"Set-up-Checkpoint-Path-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Set up Checkpoint Path</a></span></li><li><span><a href=\"#Build-model\" data-toc-modified-id=\"Build-model-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Build model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#1-Batch-Model-and-Imputing-Redactions\" data-toc-modified-id=\"1-Batch-Model-and-Imputing-Redactions-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>1 Batch Model and Imputing Redactions</a></span></li></ul></li><li><span><a href=\"#400-Sequence-Length-Model\" data-toc-modified-id=\"400-Sequence-Length-Model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>400 Sequence Length Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Set-up-Checkpoint-Path\" data-toc-modified-id=\"Set-up-Checkpoint-Path-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Set up Checkpoint Path</a></span></li><li><span><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Build Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#1-Batch-and-Redactions\" data-toc-modified-id=\"1-Batch-and-Redactions-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>1 Batch and Redactions</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "The model will be made in TensorFlow using [Eager Execution mode](https://www.tensorflow.org/guide/eager). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:32.393942Z",
     "start_time": "2019-08-13T21:11:12.865138Z"
    }
   },
   "outputs": [],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning Data\n",
    "\n",
    "\n",
    "### Load Data\n",
    "I frequently trained models using google colab so I have an option to mount and load a google drive folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:32.424209Z",
     "start_time": "2019-08-13T21:11:32.421733Z"
    }
   },
   "outputs": [],
   "source": [
    "### Option to run notebook using google colab as data storage\n",
    "drive_load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:32.484311Z",
     "start_time": "2019-08-13T21:11:32.450487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '§', '¶', 'é', 'ü', 'ſ', 'а', 'в', 'д', 'е', 'и', 'к', 'м', 'н', 'о', 'п', 'р', 'т', 'ч', 'ы', 'я', '–', '—', '‘', '’', '“', '”', '…', '■']\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    path_to_file = \"/content/gdrive/My Drive/code/redaction model/strippedtext.txt\"\n",
    "else:\n",
    "    path_to_file = \"strippedtext.txt\"\n",
    "\n",
    "\n",
    "raw_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "unfiltered_vocab = sorted(set(raw_text))\n",
    "print(unfiltered_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Raw Text\n",
    "\n",
    "Remove a bunch of specific characters from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:32.565608Z",
     "start_time": "2019-08-13T21:11:32.509874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text pre-substituion : 1320770 characters.\n",
      "120 unique characters after substitution\n",
      "\n",
      "Length of text post-substituion : 1304546 characters. \n",
      "76 unique characters after substitution. \n",
      "\n",
      "Pre-to-Post difference : 16224 characters.\n"
     ]
    }
   ],
   "source": [
    "print('Length of text pre-substituion : {} characters.'.format(len(raw_text)))\n",
    "print('{} unique characters after substitution'.format(len(unfiltered_vocab)))\n",
    "\n",
    "sub_text = re.sub('[!@#$§\\[\\]%&+*\\(\\);<>=|¶éüſавдеикмнопртчыя–—‘’“”]', '', raw_text)\n",
    "sub_text = re.sub('…', '...', sub_text)\n",
    "\n",
    "vocab = sorted(set(sub_text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('\\nLength of text post-substituion : {} characters. '.format(len(sub_text)))\n",
    "print('{} unique characters after substitution. \\n\\nPre-to-Post difference : {} characters.'.format(len(vocab), len(raw_text) - len(sub_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Mappings\n",
    "\n",
    "Create mappings from characters to indices and from indices to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:32.593819Z",
     "start_time": "2019-08-13T21:11:32.589713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '\"' :   3,\n",
      "  \"'\" :   4,\n"
     ]
    }
   ],
   "source": [
    "### Create a mapping from unique characters to indices and vice versa\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "for char,_ in zip(char2idx, range(5)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T13:05:43.879560Z",
     "start_time": "2019-08-13T13:05:43.873333Z"
    }
   },
   "source": [
    "## Creating train and test data objects\n",
    "\n",
    "Create the training and test text sets.\n",
    "- The training set will be all the text from the original report with any redactions sections removed and replaced with three periods (...).\n",
    "- The test set will be made of all the redactions in the report and some number of preceding characters to prompt the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:46.359681Z",
     "start_time": "2019-08-13T21:11:46.299455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■', 'ng from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\\r\\nIn mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■']\n"
     ]
    }
   ],
   "source": [
    "train_text = re.sub('■+', '...',sub_text)\n",
    "\n",
    "### Each redaction has the first and last index values recorded along with the \n",
    "### length of the redactions\n",
    "### Length of redactions isn't actually used anywhere\n",
    "redactions = [[m.start(), m.end(), m.end() - m.start()] for m in re.finditer('■+', sub_text)]\n",
    "\n",
    "prec_char_750 = 750\n",
    "prec_char_400 = 400\n",
    "\n",
    "test_text_750 = [sub_text[i[0] - prec_char_750 : i[1]] for i in redactions]\n",
    "test_text_400 = [sub_text[i[0] - prec_char_400 : i[1]] for i in redactions]\n",
    "\n",
    "print(test_text_400[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text into the numeric array form using the character to index mappings made earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:48.249554Z",
     "start_time": "2019-08-13T21:11:47.646970Z"
    }
   },
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in sub_text])\n",
    "train_text_as_int = np.array([char2idx[c] for c in train_text])\n",
    "test_text_750_as_int = [np.array([char2idx[c] for c in i]) for i in test_text_750]\n",
    "test_text_400_as_int = [np.array([char2idx[c] for c in i]) for i in test_text_400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation Functions\n",
    "\n",
    "Create a function to chunk up an input string, splitting it into predictor text and a target character. \n",
    "\n",
    "Batch the full text into seq_length + 1 length sequences then `map` that new function onto the sequences to produce predictors and target chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:11:55.575189Z",
     "start_time": "2019-08-13T21:11:55.570128Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "def dataset_generator(seq_length, text):\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(train_text_as_int)\n",
    "    \n",
    "    ### Increasing to 100 prevented model from \"running out of data\". \n",
    "    ### Probably part of ResourceExhausted Error but running out of data was worse.\n",
    "    repeat_n = 100\n",
    "    sequences = char_dataset.repeat(repeat_n).batch(seq_length+1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with sequence lengths 750 and 400 to test out two different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:03.081933Z",
     "start_time": "2019-08-13T21:12:03.039107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((400,), (400,)), types: (tf.int64, tf.int64)> \n",
      " <DatasetV1Adapter shapes: ((750,), (750,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "seq_length_400 = 400\n",
    "dataset_400 = dataset_generator(seq_length_400, sub_text)\n",
    "\n",
    "seq_length_750 = 750\n",
    "dataset_750 = dataset_generator(seq_length_750, sub_text)\n",
    "print(dataset_400, '\\n', dataset_750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "First I'll define some variables and functions that'll be used in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:10.706796Z",
     "start_time": "2019-08-13T21:12:10.703835Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function for the models. We'll use categorical crossentropy as that's the simplest loss function for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:18.408969Z",
     "start_time": "2019-08-13T21:12:18.405588Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate text given a starting string using a model. Code originally taken from [this](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb) TensorFlow tutorial but adapted to this use case.\n",
    "\n",
    "- Changes include:\n",
    "    - Making `temperature` a parameter\n",
    "    - Changing `num_generate` to dynamically set to the length of redacted piece of `start_string`\n",
    "    - Add a separator in the text generated to show where redaction/prediction starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:25.966778Z",
     "start_time": "2019-08-13T21:12:25.959410Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature=1):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = len(start_string) - len(re.sub('■+', '', start_string))\n",
    "    start_string = re.sub('■+', '', start_string)\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    #temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ' || ' + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 750 Sequence Length Model\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "Set parameters for 750 sequence length model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:40.943140Z",
     "start_time": "2019-08-13T21:12:40.936606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((64, 750), (64, 750)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "learning_rate_750 = 5e-4\n",
    "embedding_dim_750 = 256\n",
    "nb_epoches_750 = 1\n",
    "rnn_units_750 = 600\n",
    "load_weights_750 = True\n",
    "keep_training_750 = False\n",
    "\n",
    "### Not sure if is doing what I think but it was used in a tutorial I followed so I'm using it\n",
    "### Essentially, I want each epoch to read the document once\n",
    "examples_per_epoch_750 = len(sub_text)//seq_length_750\n",
    "steps_per_epoch_750 = int(examples_per_epoch_750 / BATCH_SIZE)\n",
    "\n",
    "### Set up dataset that's been shuffled and batched\n",
    "dataset_sb_750 = dataset_750.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_sb_750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the user is loading from drive or now, pick the folder that'll have the checkpoints in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:48.730216Z",
     "start_time": "2019-08-13T21:12:48.725257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_750_model\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    checkpoint_750_path = \"/content/gdrive/My Drive/code/redaction model/checkpoints_750_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "else:\n",
    "    checkpoint_750_path = \"checkpoints_750_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "    \n",
    "checkpoint_750_dir = os.path.dirname(checkpoint_750_path)\n",
    "\n",
    "### Make the directory if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_750_dir):\n",
    "    os.mkdir(checkpoint_750_dir)\n",
    "    \n",
    "print(checkpoint_750_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "Build the model for the 750 sequence length model. \n",
    "\n",
    "Use only 1 GRU because that's as big as Google Colab will let it get while in Eager Execution mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:12:56.306275Z",
     "start_time": "2019-08-13T21:12:56.301121Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_750(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5), \n",
    "        tf.keras.layers.Dense(2*vocab_size),\n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model object, compile it and potentially load weights to it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:13:04.269059Z",
     "start_time": "2019-08-13T21:13:03.868781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Weights loaded from :  checkpoints_750_model/epochs:001-loss:1.338.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           19456     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 600)           1542600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 152)           91352     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 76)            11628     \n",
      "=================================================================\n",
      "Total params: 1,665,036\n",
      "Trainable params: 1,665,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_750 = build_model_750(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim_750,\n",
    "    rnn_units=rnn_units_750,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "adam_opt = Adam(lr=learning_rate_750)\n",
    "\n",
    "model_750.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "most_recent_750_checkpoint = max([checkpoint_750_dir + '/' + i for i in os.listdir(checkpoint_750_dir)], key = os.path.getctime)\n",
    "\n",
    "if load_weights_750 == True:\n",
    "    model_750.load_weights(most_recent_750_checkpoint)\n",
    "    print(\"Weights loaded from : \", most_recent_750_checkpoint)\n",
    "\n",
    "model_750.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model\n",
    "\n",
    "Add callbacks then fit the model.\n",
    "- If `load_weights` == True then the user has weights they would like to load prior to training.\n",
    "- If `keep_training` == True then the user wants the model to train further after starting from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:17:12.428782Z",
     "start_time": "2019-08-13T21:16:53.842844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 18s 18s/step - loss: 1.3089\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_750_path, period=1,\n",
    "                                                         monitor='loss', save_best_only=True, \n",
    "                                                         mode='min')\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint_callback, earlystop_callback]\n",
    "\n",
    "if load_weights_750 == True:\n",
    "    if keep_training_750 == True:\n",
    "        history = model_750.fit(\n",
    "                            dataset_sb_750, \n",
    "                            epochs=nb_epoches_750, \n",
    "                            steps_per_epoch=steps_per_epoch_750, \n",
    "                            callbacks=callbacks_list)\n",
    "    elif keep_training_750 == False:\n",
    "        history = model_750.fit(\n",
    "                            dataset_sb_750, \n",
    "                            epochs=1, \n",
    "                            steps_per_epoch=1, \n",
    "                            callbacks=callbacks_list)\n",
    "else:\n",
    "    history = model_750.fit(\n",
    "                        dataset_sb_750, \n",
    "                        epochs=nb_epoches_750, \n",
    "                        steps_per_epoch=steps_per_epoch_750, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Batch Model and Imputing Redactions\n",
    "\n",
    "Build a model with 1 batch size and load weights from above cell's checkpoints. \n",
    "\n",
    "Using this 1batch model, generate predictions for the first 5 redactions of the Mueller report.\n",
    "- Generate three predictions with varying temperatures per redaction.\n",
    "    - Lower temperature gives less predictable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:23:05.828312Z",
     "start_time": "2019-08-13T21:22:20.280117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_750_model/epochs:001-loss:1.338.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, None, 256)            19456     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 600)            1542600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, None, 152)            91352     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (1, None, 76)             11628     \n",
      "=================================================================\n",
      "Total params: 1,665,036\n",
      "Trainable params: 1,665,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "750 Sequence Length predictors\n",
      "\n",
      "################################################\n",
      "\n",
      "0\n",
      "\n",
      "### : Original Text\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "\n",
      "### : Temperature == 1 prediction\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || the 2013 objection of GRU callses, and muricyess, the investigation soming he project, and atherwifes, forergignon-was suched individu\n",
      "\n",
      "### : Temperature == .6 prediction\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || Papadopoulos had documents and Sessions affered to provide who to deford the Presidents personal counsel. There wither interviews with\n",
      "\n",
      "### : Temperature == 1.3 prediction\n",
      "ion into Russias interference in the 2016 presidential election and related matters, and his actions towards the Special Counsels investigation. Volume II separately states its framework and the considerations that guided that investigation.\n",
      "EXECUTIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || Onalliancy saorshing, Is BaurPopo, UDCCLUREY52ENTS\n",
      "PUSEAETEV_013/14/17/ 6/1/17 302, at 3 Hicks 1,/23/17 302, at 6.\n",
      "730 the Trump Tra\n",
      "\n",
      "################################################\n",
      "\n",
      "1\n",
      "\n",
      "### : Original Text\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "\n",
      "### : Temperature == 1 prediction\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || . aboldote CREINQ AGTRNCKNOSTTUPTATKIN\n",
      " White Vlistone AbDonald Gerson 114:9417 submitted.\n",
      "577 Page-8/13/88 302, at 16-25\n",
      "IV., ...\n",
      "796 12 \n",
      ". 2399 Gourgn time.\" 983\n",
      "1013 had Me2can-Maraforted\n",
      "A balled to al February 11, 2017 Section 1S115241.\n",
      "Trump Ormanizations notagizent an admictover to him. 1990\n",
      "615 15/11/16 Text Messages, President.\n",
      "410 GGU tome questions IRF, Papployerves Seneet out thost set. MSGRGACAN 551/13/17/17 Text Messages, Intending, Comey connected over to stot re\n",
      "\n",
      "### : Temperature == .6 prediction\n",
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || government officials and foreign policy advisory told him the President and the President beloved then preceding and supported speech. 147 The Presidents conversations. 476 Polictian Election Intelligence Committee, 115th Corgent Horter March 2017, Pollinct, and confurms and Special Counsels Office to President-Elect Trump seen legal and the President was not elections had informed the Presidents specifically and atternized to Russia. The President Manaforts Section 1515, stiting to be\n",
      "\n",
      "### : Temperature == 1.3 prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIVE SUMMARY TO VOLUME I\n",
      "RUSSIAN SOCIAL MEDIA CAMPAIGN\n",
      "The Internet Research Agency IRA carried out the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || intimeberg hrow Russia but foull bring elable dury-on, Kisconslaks, Hujry, .. 338 NHSC-HOag.\n",
      "399 10891 698 Fvann 3019429 pollings officians, woulf, stweem\" Timear? Huet.\" 474 NE3SO32641096\":5649 Bunt 2/3/17 302, at 112\n",
      "Senate Flynns couroxizRe0paege CVisen, ..., UrSaterate Krommphetmees JRS.\n",
      "3.3, soinN? 729\n",
      "A. Avussions after Stame and McGranhleg, Pages sforHelugher conlectanciS.\n",
      " ScGeO\n",
      "5 and Riks0 Coongele Gageb-KiAlment Sebst 2019 he, justreirnchels Brieded, the Russian Embowski\n",
      "\n",
      "################################################\n",
      "\n",
      "2\n",
      "\n",
      "### : Original Text\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016, ■■■■■■■■■■■■■■■■■■■■■■■\n",
      "\n",
      "### : Temperature == 1 prediction\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || the President pressance\n",
      "\n",
      "### : Temperature == .6 prediction\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || Clinton Campaign Trump \n",
      "\n",
      "### : Temperature == 1.3 prediction\n",
      "6, the GRU hacked into the computer networks of the Democratic Congressional Campaign Committee DCCC and the Democratic National Committee DNC. The GRU stole hundreds of thousands of documents from the compromised email accounts and networks. Around the time that the DNC announced in mid-June 2016 the Russian governments role in hacking its network, the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || C-NNg. Leares a, DLT. H\n"
     ]
    }
   ],
   "source": [
    "print(most_recent_750_checkpoint)\n",
    "\n",
    "model_750_1batch = build_model_750(vocab_size, embedding_dim_750, rnn_units_750, batch_size=1)\n",
    "model_750_1batch.load_weights(most_recent_750_checkpoint)\n",
    "model_750_1batch.build(tf.TensorShape([1, None]))\n",
    "model_750_1batch.summary()\n",
    "\n",
    "print(\"750 Sequence Length predictors\")\n",
    "for index, redaction in enumerate(test_text_750[:3]):\n",
    "    print(\"\\n################################################\\n\")\n",
    "    print(index)\n",
    "    print(\"\\n### : Original Text\")\n",
    "    print(redaction)\n",
    "    print(\"\\n### : Temperature == 1 prediction\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=1))\n",
    "    print(\"\\n### : Temperature == .6 prediction\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=.6))\n",
    "    print(\"\\n### : Temperature == 1.3 prediction\")\n",
    "    print(generate_text(model_750_1batch, start_string=redaction, temperature=1.3))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 Sequence Length Model\n",
    "\n",
    "\n",
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:24:07.309378Z",
     "start_time": "2019-08-13T21:24:07.304563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((64, 400), (64, 400)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "learning_rate_400 = 5e-4\n",
    "embedding_dim_400 = 256\n",
    "nb_epoches_400 = 1\n",
    "rnn_units_400 = 600\n",
    "load_weights_400 = True\n",
    "keep_training_400 = False\n",
    "examples_per_epoch_400 = len(sub_text)//seq_length_400\n",
    "steps_per_epoch_400 = int(examples_per_epoch_400 / BATCH_SIZE)\n",
    "\n",
    "\n",
    "dataset_sb_400 = dataset_400.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset_sb_400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:24:14.972999Z",
     "start_time": "2019-08-13T21:24:14.968302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_400_model\n"
     ]
    }
   ],
   "source": [
    "if drive_load == True:\n",
    "    checkpoint_400_path = \"/content/gdrive/My Drive/code/redaction model/checkpoints_400_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "else:\n",
    "    checkpoint_400_path = \"checkpoints_400_model/epochs:{epoch:03d}-loss:{loss:.3f}.hdf5\"\n",
    "    \n",
    "checkpoint_400_dir = os.path.dirname(checkpoint_400_path)\n",
    "\n",
    "if not os.path.isdir(checkpoint_400_dir):\n",
    "    os.mkdir(checkpoint_400_dir)\n",
    "    \n",
    "print(checkpoint_400_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:24:23.263770Z",
     "start_time": "2019-08-13T21:24:23.256259Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_400(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5),         \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5), \n",
    "        tf.keras.layers.Dense(2*vocab_size),\n",
    "        tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:24:31.574489Z",
     "start_time": "2019-08-13T21:24:31.076560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from :  checkpoints_400_model/epochs_029-loss_0.953.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (64, None, 256)           19456     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (64, None, 600)           1542600   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (64, None, 600)           2161800   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (64, None, 600)           0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (64, None, 152)           91352     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (64, None, 76)            11628     \n",
      "=================================================================\n",
      "Total params: 3,826,836\n",
      "Trainable params: 3,826,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_400 = build_model_400(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim_400,\n",
    "    rnn_units=rnn_units_400,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "adam_opt = Adam(lr=learning_rate_400)\n",
    "\n",
    "model_400.compile(optimizer=adam_opt, loss=loss)\n",
    "\n",
    "most_recent_400_checkpoint = max([checkpoint_400_dir + '/' + i for i in os.listdir(checkpoint_400_dir)], key = os.path.getctime)\n",
    "\n",
    "if load_weights_400 == True:\n",
    "    model_400.load_weights(most_recent_400_checkpoint)\n",
    "    print(\"Weights loaded from : \", most_recent_400_checkpoint)\n",
    "\n",
    "model_400.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:25:03.750815Z",
     "start_time": "2019-08-13T21:24:39.518056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 24s 24s/step - loss: 0.9504\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_400_path, period=1,\n",
    "                                                         monitor='loss', save_best_only=True, \n",
    "                                                         mode='min')\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint_callback, earlystop_callback]\n",
    "\n",
    "if load_weights_400 == True:\n",
    "    if keep_training_400 == True:\n",
    "        history = model_400.fit(\n",
    "                            dataset_sb_400, \n",
    "                            epochs=nb_epoches_400, \n",
    "                            steps_per_epoch=steps_per_epoch_400, \n",
    "                            callbacks=callbacks_list)\n",
    "    elif keep_training_400 == False:\n",
    "        history = model_400.fit(\n",
    "                            dataset_sb_400, \n",
    "                            epochs=1, \n",
    "                            steps_per_epoch=1, \n",
    "                            callbacks=callbacks_list)\n",
    "else:\n",
    "    history = model_400.fit(\n",
    "                        dataset_sb_400, \n",
    "                        epochs=nb_epoches_400, \n",
    "                        steps_per_epoch=steps_per_epoch_400, \n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Batch and Redactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T21:27:55.347141Z",
     "start_time": "2019-08-13T21:26:53.327150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_400_model/epochs_029-loss_0.953.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, None, 256)            19456     \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (1, None, 600)            1542600   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (1, None, 600)            2161800   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (1, None, 600)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (1, None, 152)            91352     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (1, None, 76)             11628     \n",
      "=================================================================\n",
      "Total params: 3,826,836\n",
      "Trainable params: 3,826,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "400 Sequence Length predictors\n",
      "\n",
      "################################################\n",
      "\n",
      "0\n",
      "###\n",
      "the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "### : Temperature == 1 prediction\n",
      "the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || including those changes, but During the communications is subject to such a criminal contacts, the subsertent of our United States , 7\n",
      "### : Temperature == 0.6 prediction\n",
      "the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || who supported on June 9, 2016, were never and the President was the presidential campaign, including which Gordon stoped the same with\n",
      "### : Temperature == 1.3 prediction\n",
      "the earliest Russian interference operations identified by the investigation  a social media campaign designed to provoke and amplify political and social discord in the United States. The IRA was based in St. Petersburg, Russia, and received funding from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin,  || aro n\n",
      "said he wascuts the previous followers, the Special Counsels from the Semach 1f neserves, the meanh  uelove memoeC occurred. Be\n",
      "\n",
      "################################################\n",
      "\n",
      "1\n",
      "###\n",
      "ng from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "### : Temperature == 1 prediction\n",
      "ng from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || about the story with the guys for U.S. griever than defensing a control of 5:17 a.m..\n",
      "685 Telling Wessiling .koul, Trump Jr.s D.C. Carconal and Russian Direct September 2017 ... Gerson would not recall lears to provide business wlack 909. You Tand discussions and ? After Cohens content on those date of the steps.\n",
      "on his Meeting Feb. 23, 2019 Finally Flend SC_AR_10115 Presidents Priebus 4/33/17 302, at 3.\n",
      "328 See, e.g., 8/11/17 Email, McFarland to Flynn. 11/35/17 Email, I told Kelly \n",
      "### : Temperature == 0.6 prediction\n",
      "ng from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || as the offer of such as election interference in the 2016 presidential election. 1154 According to Trump Jr. because he was allegations the presidents conduct by the Trump Tower Moscow project with the President, as described in Volume I, Section IV.A.3, supra .\n",
      "289 Comey 11/15/17 302, at 11 McGahn 12/12/17 302, at 15 Donaldson 11/6/17 302, at 17.\n",
      "898 Hant-000095 Comey 1/15/17 Memorandum, at 1 McGahn 11/12/17 302, at 10.\n",
      "795 Simes 3/8/18 302, at 10.\n",
      "797 Porter 4/13/18 302, at 17 Co\n",
      "### : Temperature == 1.3 prediction\n",
      "ng from Russian oligarch Yevgeniy Prigozhin and companies he controlled. Prigozhin is widely reported to have ties to Russian President Vladimir Putin, \n",
      "In mid-2014, the IRA sent employees to the United States on an intelligence-gathering mission with instructions  || would trop the waventy on orgaining investigation has coopedating work about a step supports emails focused to boing, and I dont kete sought to Prevengm  1947 with resignatp, telliy had obsadrante, White House sourd used the againnt on other bccapegr.\", acts, fortaldid reisters? Why days after curdination declined. February 6, 2016.\n",
      "438 Porter 4/13/18 302, at 29\n",
      "409 Parlien HerontEroled Tixi, an November 28, 2018 Crhinh Sadders Mueller Fopesmar for Text. Cloom Doush Dan Paul Manafort\n",
      "\n",
      "################################################\n",
      "\n",
      "2\n",
      "###\n",
      ", the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016, ■■■■■■■■■■■■■■■■■■■■■■■\n",
      "### : Temperature == 1 prediction\n",
      ", the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || which he is above conne\n",
      "### : Temperature == 0.6 prediction\n",
      ", the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || the President recalled \n",
      "### : Temperature == 1.3 prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", the GRU began disseminating stolen materials through the fictitious online personas DCLeaks and Guccifer 2.0. The GRU later released additional materials through the organization WikiLeaks.\r\n",
      "The presidential campaign of Donald J. Trump Trump Campaign or Campaign\" showed interest in WikiLeakss releases of documents and welcomed their potential to damage candidate Clinton. Beginning in June 2016,  || Dvitrievs obiding trasc\n"
     ]
    }
   ],
   "source": [
    "print(most_recent_400_checkpoint)\n",
    "\n",
    "model_400_1batch = build_model_400(vocab_size, embedding_dim_400, rnn_units_400, batch_size=1)\n",
    "model_400_1batch.load_weights(most_recent_400_checkpoint)\n",
    "model_400_1batch.build(tf.TensorShape([1, None]))\n",
    "model_400_1batch.summary()\n",
    "\n",
    "print(\"400 Sequence Length predictors\")\n",
    "for index, redaction in enumerate(test_text_400[:3]):\n",
    "    print(\"\\n################################################\\n\")\n",
    "    print(index)\n",
    "    print(\"###\")\n",
    "    print(redaction)\n",
    "    print(\"### : Temperature == 1 prediction\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction, temperature=1))\n",
    "    print(\"### : Temperature == 0.6 prediction\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction, temperature=0.6))\n",
    "    print(\"### : Temperature == 1.3 prediction\")\n",
    "    print(generate_text(model_400_1batch, start_string=redaction, temperature=1.3))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
